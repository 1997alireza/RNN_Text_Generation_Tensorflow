{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LUipvr-wKlx",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "CojHsL-QwV_Z",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "class ModelNetwork:\n",
    "    \"\"\"\n",
    "    RNN with num_layers LSTM layers and a fully-connected output layer\n",
    "    The network allows for a dynamic number of iterations, depending on the\n",
    "    inputs it receives.\n",
    "\n",
    "       out   (fc layer; out_size)\n",
    "        ^\n",
    "       lstm\n",
    "        ^\n",
    "       lstm  (lstm size)\n",
    "        ^\n",
    "        in   (in_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_size, lstm_size, num_layers, out_size, session,\n",
    "                 learning_rate=0.003, name=\"rnn\"):\n",
    "        self.scope = name\n",
    "        self.in_size = in_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.out_size = out_size\n",
    "        self.session = session\n",
    "        self.learning_rate = tf.constant(learning_rate)\n",
    "        # Last state of LSTM, used when running the network in TEST mode\n",
    "        self.lstm_last_state = np.zeros(\n",
    "            (self.num_layers * 2 * self.lstm_size,)\n",
    "        )\n",
    "        with tf.variable_scope(self.scope):\n",
    "            # (batch_size, timesteps, in_size)\n",
    "            self.xinput = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(None, None, self.in_size),\n",
    "                name=\"xinput\"\n",
    "            )\n",
    "            self.lstm_init_value = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(None, self.num_layers * 2 * self.lstm_size),\n",
    "                name=\"lstm_init_value\"\n",
    "            )\n",
    "            # LSTM\n",
    "            self.lstm_cells = [\n",
    "                tf.contrib.rnn.BasicLSTMCell(\n",
    "                    self.lstm_size,\n",
    "                    forget_bias=1.0,\n",
    "                    state_is_tuple=False\n",
    "                ) for i in range(self.num_layers)\n",
    "            ]\n",
    "            self.lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "                self.lstm_cells,\n",
    "                state_is_tuple=False\n",
    "            )\n",
    "            # Iteratively compute output of recurrent network\n",
    "            outputs, self.lstm_new_state = tf.nn.dynamic_rnn(\n",
    "                self.lstm,\n",
    "                self.xinput,\n",
    "                initial_state=self.lstm_init_value,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            # Linear activation (FC layer on top of the LSTM net)\n",
    "            self.rnn_out_W = tf.Variable(\n",
    "                tf.random_normal(\n",
    "                    (self.lstm_size, self.out_size),\n",
    "                    stddev=0.01\n",
    "                )\n",
    "            )\n",
    "            self.rnn_out_B = tf.Variable(\n",
    "                tf.random_normal(\n",
    "                    (self.out_size,), stddev=0.01\n",
    "                )\n",
    "            )\n",
    "            outputs_reshaped = tf.reshape(outputs, [-1, self.lstm_size])\n",
    "            network_output = tf.matmul(\n",
    "                outputs_reshaped,\n",
    "                self.rnn_out_W\n",
    "            ) + self.rnn_out_B\n",
    "            batch_time_shape = tf.shape(outputs)\n",
    "            self.final_outputs = tf.reshape(\n",
    "                tf.nn.softmax(network_output),\n",
    "                (batch_time_shape[0], batch_time_shape[1], self.out_size)\n",
    "            )\n",
    "            # Training: provide target outputs for supervised training.\n",
    "            self.y_batch = tf.placeholder(\n",
    "                tf.float32,\n",
    "                (None, None, self.out_size)\n",
    "            )\n",
    "            y_batch_long = tf.reshape(self.y_batch, [-1, self.out_size])\n",
    "            self.cost = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=network_output,\n",
    "                    labels=y_batch_long\n",
    "                )\n",
    "            )\n",
    "            self.train_op = tf.train.RMSPropOptimizer(\n",
    "                self.learning_rate,\n",
    "                0.9\n",
    "            ).minimize(self.cost)\n",
    "\n",
    "    # Input: X is a single element, not a list!\n",
    "    def run_step(self, x, init_zero_state=True):\n",
    "        # Reset the initial state of the network.\n",
    "        if init_zero_state:\n",
    "            init_value = np.zeros((self.num_layers * 2 * self.lstm_size,))\n",
    "        else:\n",
    "            init_value = self.lstm_last_state\n",
    "        out, next_lstm_state = self.session.run(\n",
    "            [self.final_outputs, self.lstm_new_state],\n",
    "            feed_dict={\n",
    "                self.xinput: [x],\n",
    "                self.lstm_init_value: [init_value]\n",
    "            }\n",
    "        )\n",
    "        self.lstm_last_state = next_lstm_state[0]\n",
    "        return out[0][0]\n",
    "\n",
    "    # xbatch must be (batch_size, timesteps, input_size)\n",
    "    # ybatch must be (batch_size, timesteps, output_size)\n",
    "    def train_batch(self, xbatch, ybatch):\n",
    "        init_value = np.zeros(\n",
    "            (xbatch.shape[0], self.num_layers * 2 * self.lstm_size)\n",
    "        )\n",
    "        cost, _ = self.session.run(\n",
    "            [self.cost, self.train_op],\n",
    "            feed_dict={\n",
    "                self.xinput: xbatch,\n",
    "                self.y_batch: ybatch,\n",
    "                self.lstm_init_value: init_value\n",
    "            }\n",
    "        )\n",
    "        return cost\n",
    "\n",
    "\n",
    "def embed_to_vocab(data_, vocab):\n",
    "    \"\"\"\n",
    "    Embed string to character-arrays -- it generates an array len(data)\n",
    "    x len(vocab).\n",
    "\n",
    "    Vocab is a list of elements.\n",
    "    \"\"\"\n",
    "    data = np.zeros((len(data_), len(vocab)))\n",
    "    cnt = 0\n",
    "    for s in data_:\n",
    "        v = [0.0] * len(vocab)\n",
    "        v[vocab.index(s)] = 1.0\n",
    "        data[cnt, :] = v\n",
    "        cnt += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "def decode_embed(array, vocab):\n",
    "    return vocab[array.index(1)]\n",
    "\n",
    "\n",
    "def load_data(input):\n",
    "    # Load the data\n",
    "    data_ = \"\"\n",
    "    with open(input, 'r') as f:\n",
    "        data_ += f.read()\n",
    "    data_ = data_.lower()\n",
    "    # Convert to 1-hot coding\n",
    "    vocab = sorted(list(set(data_)))\n",
    "    data = embed_to_vocab(data_, vocab)\n",
    "    return data, vocab\n",
    "\n",
    "\n",
    "def check_restore_parameters(sess, saver):\n",
    "    \"\"\" Restore the previously trained parameters if there are any. \"\"\"\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('saved/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "\n",
    "def main(m_mode=\"train\", m_prefix=\"The \"):\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\n",
    "    #     \"--input_file\",\n",
    "    #     type=str,\n",
    "    #     default=\"data/shakespeare.txt\",\n",
    "    #     help=\"Text file to load.\"\n",
    "    # )\n",
    "    # parser.add_argument(\n",
    "    #     \"--test_prefix\",\n",
    "    #     type=str,\n",
    "    #     default=\"The \",\n",
    "    #     help=\"Test text prefix to train the network.\"\n",
    "    # )\n",
    "    # parser.add_argument(\n",
    "    #     \"--ckpt_file\",\n",
    "    #     type=str,\n",
    "    #     default=\"saved/model.ckpt\",\n",
    "    #     help=\"Model checkpoint file to load.\"\n",
    "    # )\n",
    "    # parser.add_argument(\n",
    "    #     \"--mode\",\n",
    "    #     type=str,\n",
    "    #     default=\"train\",\n",
    "    #     choices=set((\"talk\", \"train\")),\n",
    "    #     help=\"Execution mode: talk or train.\"\n",
    "    # )\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    class args:\n",
    "        ckpt_file = \"saved/model.ckpt\"\n",
    "        test_prefix = m_prefix\n",
    "        mode = m_mode\n",
    "        input_file = \"shakespeare.txt\"\n",
    "\n",
    "    ckpt_file = None\n",
    "    TEST_PREFIX = args.test_prefix  # Prefix to prompt the network in test mode\n",
    "\n",
    "    if args.ckpt_file:\n",
    "        ckpt_file = args.ckpt_file\n",
    "\n",
    "    # Load the data\n",
    "    data, vocab = load_data(args.input_file)\n",
    "\n",
    "    in_size = out_size = len(vocab)\n",
    "    lstm_size = 256  # 128\n",
    "    num_layers = 2\n",
    "    batch_size = 64  # 128\n",
    "    time_steps = 100  # 50\n",
    "\n",
    "    NUM_TRAIN_BATCHES = 20000\n",
    "\n",
    "    # Number of test characters of text to generate after training the network\n",
    "    LEN_TEST_TEXT = 500\n",
    "\n",
    "    # Initialize the network\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "    net = ModelNetwork(\n",
    "        in_size=in_size,\n",
    "        lstm_size=lstm_size,\n",
    "        num_layers=num_layers,\n",
    "        out_size=out_size,\n",
    "        session=sess,\n",
    "        learning_rate=0.003,\n",
    "        name=\"char_rnn_network\"\n",
    "    )\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    # 1) TRAIN THE NETWORK\n",
    "    if args.mode == \"train\":\n",
    "        check_restore_parameters(sess, saver)\n",
    "        last_time = time.time()\n",
    "        batch = np.zeros((batch_size, time_steps, in_size))\n",
    "        batch_y = np.zeros((batch_size, time_steps, in_size))\n",
    "        possible_batch_ids = range(data.shape[0] - time_steps - 1)\n",
    "\n",
    "        for i in range(NUM_TRAIN_BATCHES):\n",
    "            # Sample time_steps consecutive samples from the dataset text file\n",
    "            batch_id = random.sample(possible_batch_ids, batch_size)\n",
    "\n",
    "            for j in range(time_steps):\n",
    "                ind1 = [k + j for k in batch_id]\n",
    "                ind2 = [k + j + 1 for k in batch_id]\n",
    "\n",
    "                batch[:, j, :] = data[ind1, :]\n",
    "                batch_y[:, j, :] = data[ind2, :]\n",
    "\n",
    "            cst = net.train_batch(batch, batch_y)\n",
    "\n",
    "            if (i % 100) == 0:\n",
    "                new_time = time.time()\n",
    "                diff = new_time - last_time\n",
    "                last_time = new_time\n",
    "                print(\"batch: {}  loss: {}  speed: {} batches / s\".format(\n",
    "                    i, cst, 100 / diff\n",
    "                ))\n",
    "                saver.save(sess, ckpt_file)\n",
    "    elif args.mode == \"talk\":\n",
    "        # 2) GENERATE LEN_TEST_TEXT CHARACTERS USING THE TRAINED NETWORK\n",
    "        saver.restore(sess, ckpt_file)\n",
    "\n",
    "        TEST_PREFIX = TEST_PREFIX.lower()\n",
    "        for i in range(len(TEST_PREFIX)):\n",
    "            out = net.run_step(embed_to_vocab(TEST_PREFIX[i], vocab), i == 0)\n",
    "\n",
    "        print(\"Sentence:\")\n",
    "        gen_str = TEST_PREFIX\n",
    "        for i in range(LEN_TEST_TEXT):\n",
    "            # Sample character from the network according to the generated\n",
    "            # output probabilities.\n",
    "            element = np.random.choice(range(len(vocab)), p=out)\n",
    "            gen_str += vocab[element]\n",
    "            out = net.run_step(embed_to_vocab(vocab[element], vocab), False)\n",
    "\n",
    "        print(gen_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "sFKIJoiPwX_1",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1040.0
    },
    "outputId": "c1f41fec-f162-45fa-d9c0-0d206dfd6676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f1d5b5b6ba8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f1d5b5b6cc0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:From <ipython-input-4-9b588cb3b281>:91: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "batch: 0  loss: 3.6644158363342285  speed: 68.16923924319751 batches / s\n",
      "batch: 100  loss: 3.1210591793060303  speed: 0.8255809197662617 batches / s\n",
      "batch: 200  loss: 3.0577924251556396  speed: 0.8196110365356254 batches / s\n",
      "batch: 300  loss: 2.3579447269439697  speed: 0.8232900212099784 batches / s\n",
      "batch: 400  loss: 2.0213863849639893  speed: 0.8230272567812452 batches / s\n",
      "batch: 500  loss: 1.8043999671936035  speed: 0.8186376471012944 batches / s\n",
      "batch: 600  loss: 1.6617120504379272  speed: 0.8199870770879335 batches / s\n",
      "batch: 700  loss: 1.538399338722229  speed: 0.8247475824608316 batches / s\n",
      "batch: 800  loss: 1.4233694076538086  speed: 0.8171780891327086 batches / s\n",
      "batch: 900  loss: 1.4351468086242676  speed: 0.8135597444233489 batches / s\n",
      "batch: 1000  loss: 1.3942102193832397  speed: 0.8028182090671986 batches / s\n",
      "batch: 1100  loss: 1.3688507080078125  speed: 0.8096071123829167 batches / s\n",
      "batch: 1200  loss: 1.323742389678955  speed: 0.8100965542120209 batches / s\n",
      "batch: 1300  loss: 1.3161016702651978  speed: 0.8126164466022585 batches / s\n",
      "batch: 1400  loss: 1.3209530115127563  speed: 0.8221247956690673 batches / s\n",
      "batch: 1500  loss: 1.2988742589950562  speed: 0.8147030887710481 batches / s\n",
      "batch: 1600  loss: 1.267621397972107  speed: 0.81986164115475 batches / s\n",
      "batch: 1700  loss: 1.2947360277175903  speed: 0.8169647668459321 batches / s\n",
      "batch: 1800  loss: 1.277204155921936  speed: 0.8131326550556301 batches / s\n",
      "batch: 1900  loss: 1.2456603050231934  speed: 0.8186361307880472 batches / s\n",
      "batch: 2000  loss: 1.2121080160140991  speed: 0.8066172374218101 batches / s\n",
      "batch: 2100  loss: 1.2413612604141235  speed: 0.8217729164304861 batches / s\n",
      "batch: 2200  loss: 1.1820217370986938  speed: 0.8156708836251705 batches / s\n",
      "batch: 2300  loss: 1.2008156776428223  speed: 0.8108499182202874 batches / s\n",
      "batch: 2400  loss: 1.1667088270187378  speed: 0.8160206798349875 batches / s\n",
      "batch: 2500  loss: 1.1883009672164917  speed: 0.8135720864895648 batches / s\n",
      "batch: 2600  loss: 1.1775808334350586  speed: 0.8206919859722168 batches / s\n",
      "batch: 2700  loss: 1.1835181713104248  speed: 0.8226343731848414 batches / s\n",
      "batch: 2800  loss: 1.1578495502471924  speed: 0.8193642121104162 batches / s\n",
      "batch: 2900  loss: 1.1461583375930786  speed: 0.8205467608343487 batches / s\n",
      "batch: 3000  loss: 1.159798502922058  speed: 0.8206989794361693 batches / s\n",
      "batch: 3100  loss: 1.0920836925506592  speed: 0.8190128811508096 batches / s\n",
      "batch: 3200  loss: 1.0801305770874023  speed: 0.8241953611571188 batches / s\n",
      "batch: 3300  loss: 1.0502095222473145  speed: 0.8148583836035115 batches / s\n",
      "batch: 3400  loss: 1.0870569944381714  speed: 0.8208160264404311 batches / s\n",
      "batch: 3500  loss: 1.0878691673278809  speed: 0.8162828998618634 batches / s\n",
      "batch: 3600  loss: 1.063854694366455  speed: 0.8229236168571905 batches / s\n",
      "batch: 3700  loss: 1.1014734506607056  speed: 0.823398698121188 batches / s\n",
      "batch: 3800  loss: 1.080612063407898  speed: 0.8174526709669994 batches / s\n",
      "batch: 3900  loss: 1.0815016031265259  speed: 0.8249436005363617 batches / s\n",
      "batch: 4000  loss: 1.0942586660385132  speed: 0.8300954305438444 batches / s\n",
      "batch: 4100  loss: 1.0091415643692017  speed: 0.8265418141662132 batches / s\n",
      "batch: 4200  loss: 1.0655121803283691  speed: 0.8303320756377062 batches / s\n",
      "batch: 4300  loss: 1.0428892374038696  speed: 0.8260743747474959 batches / s\n",
      "batch: 4400  loss: 1.0449442863464355  speed: 0.8324087564435507 batches / s\n",
      "batch: 4500  loss: 1.0630897283554077  speed: 0.8295618759255253 batches / s\n",
      "batch: 4600  loss: 1.0488052368164062  speed: 0.8281384607170962 batches / s\n",
      "batch: 4700  loss: 0.9959841966629028  speed: 0.8353832465719723 batches / s\n",
      "batch: 4800  loss: 1.0230097770690918  speed: 0.830329092187497 batches / s\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "q6N-yXKguor6",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
